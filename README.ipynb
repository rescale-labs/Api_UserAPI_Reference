{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User API\n",
    "\n",
    "this and that\n",
    "\n",
    "## Starting the notebook\n",
    "\n",
    "The only prerequisite is a working installation of Python 3.8+.\n",
    "\n",
    "```\n",
    "$ python -m venv .venv\n",
    "$ . .venv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "$ jupyter notebook UserAPI_Reference.ipynb\n",
    "```\n",
    "\n",
    "Cells need to executed in order as there are mane cells further down the line \n",
    "\n",
    "### Python and JSON\n",
    "\n",
    "Note on how python dictionaries are 1-1 mapped to JSON.\n",
    "\n",
    "## Authorization\n",
    "\n",
    "Generate API key.\n",
    "Never store it in a code repository.\n",
    "\n",
    "Rescale tools like Rescale CLI look for `apiconfig` authorization configuration file in users home directory.\n",
    "\n",
    "```\n",
    "$HOME/.config/rescale/apiconfig             # Linux\n",
    "%USERPROFILE%\\config\\rescale\\apiconfig      # Windows\n",
    "```\n",
    "\n",
    "We will use this convention to store our credentials. Create the `apiconfig` text file in the abovementioned location and fill it with the following lines\n",
    "\n",
    "```\n",
    "[default]\n",
    "apibaseurl = https://eu.rescale.com\n",
    "apikey = 79a49b3132335a44742e86c9126e5cfaa1ea2489\n",
    "```\n",
    "\n",
    "If you need to execute your script on several platforms you can define alternative profiles (fo example `[us]`). The [config.py](config.py) module is provided for your convenience. You can copy it next to your scripts and use it as a standard way to retrieve credentials. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "apiurl, apikey = config.get_profile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our `apikey` and a base URL for our API calls. Let's make the first call to get our user details. To make REST API calls we will use the [`requests`](https://requests.readthedocs.io/en/latest/) module, defiled as a dependency in the [`requirements.txt`](requirements.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "results = requests.get(f\"{apiurl}/api/v2/users/me/\")\n",
    "if results.status_code != 200:\n",
    "    print(results.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got status code `401 Unauthorized` which suggests that we're missing our credentials. Let's define `headers` that we will use to authorize future API calls. Instead of checking for result code, let's use a function that raises exception for all statuses that signify lack of success. Finally, we pretty-print the response JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"Token {apikey}\"}\n",
    "results = requests.get(f\"{apiurl}/api/v2/users/me/\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(results.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to proceed and build up job submission.\n",
    "\n",
    "## Coretypes and Analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(f\"{apiurl}/api/v2/coretypes/\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "# Display the total count of coretypes and the amount present in the response\n",
    "print(f\"Count: {results.json()['count']}; Length: {len(results.json()['results'])}\")\n",
    "\n",
    "# Display refernece to the next page\n",
    "print(f\"Next page: {results.json()['next']}\")\n",
    "\n",
    "# Display a coretype object and its properties\n",
    "pprint(results.json()[\"results\"][0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that the response returned total count of coretypes that is larger the the total count of coretype objects in the `results` list. This will happen often for endpoints that patentially return large amoutnts of data. In such siuuations results are paged and each page contains a refrence to the `next` page.\n",
    "\n",
    "Since paging is common, let's define a function that will loop through all the pages and aggregate objects returned in the `results` list of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_result_pages(url, headers={}, params={}):\n",
    "    results = []\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    results.extend(res.json()[\"results\"])\n",
    "\n",
    "    while res.json()[\"next\"] != None:\n",
    "        res = requests.get(res.json()[\"next\"], headers=headers)\n",
    "        res.raise_for_status()\n",
    "        results.extend(res.json()[\"results\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "coretypes = get_all_result_pages(f\"{apiurl}/api/v2/coretypes/\", headers)\n",
    "\n",
    "print(f\"Length: {len(coretypes)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now confident that we fetched all coretypes. For our simple job we will need a general purpose coretype with a low corecount. Let's list coretype `code`s in the `general` category together with their `cores` counts and `processorInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_coretypes = {\n",
    "    c['code']: {'cores': c['cores'], 'processorInfo': c['processorInfo']}\n",
    "    for c in coretypes\n",
    "    if \"general\" in c[\"categoryCodes\"]\n",
    "}\n",
    "\n",
    "pprint(general_coretypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a shortlist we need to decide which coretype to use. Since our test calcuation does not have specific requirements, we will use the least expensive option. Let's fetch coretype prices, filter prices for on-demand economy (ODE) an `linux` os, link them with general purpose coretypes and get the minum price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prices\n",
    "results = requests.get(f\"{apiurl}/api/v2/billing/computeprices/\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "# Print sample pricing object\n",
    "pprint(results.json()[-1])\n",
    "\n",
    "# Filter out active ODE linux based coretypes\n",
    "linux_ode_coretypes = {\n",
    "    c[\"coreType\"]: float(c[\"value\"][\"amount\"])\n",
    "    for c in results.json()\n",
    "    if c[\"planType\"].endswith(\"on-demand\")\n",
    "    and c[\"os\"] == \"linux\"\n",
    "    and c[\"isActive\"]\n",
    "    and c[\"coreType\"] in general_coretypes\n",
    "}\n",
    "\n",
    "# Find the least expensive one\n",
    "min_price_coretype = min(linux_ode_coretypes, key=linux_ode_coretypes.get)\n",
    "print(f\"Lowest price coretype: {min_price_coretype} at {linux_ode_coretypes[min_price_coretype]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot market prices are dynamic and therefore your cheapest coretype may change even during a day. Let's assume that `granite` is the coretype we want to use.\n",
    "\n",
    "Now, that we know which coretype to use, we need to select the analysis. In order to build our job create request, we need to know analysis and version codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses = get_all_result_pages(f\"{apiurl}/api/v2/analyses/\", headers)\n",
    "\n",
    "versions_count = 0\n",
    "for a in analyses:\n",
    "    versions_count += len(a[\"versions\"])\n",
    "\n",
    "print(\n",
    "    f\"Total number of analyses: {len(analyses)}, Total number of versions: {versions_count}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample analysis code: {analyses[0]['code']} and version code: {analyses[0]['versions'][0]['versionCode']}\"\n",
    ")\n",
    "\n",
    "pprint(analyses[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your workspace does not have software filters in place, you should see over 700 analyses and over 3200 unique versions. It may be cumbersome to find the code and version that you need. A more practical approach is to create a job using the web portal and then query Rescale API to get the JSON. We will do it in the next section.\n",
    "\n",
    "## Jobs and Files\n",
    "\n",
    "Go to the portal and get a Job ID for a job similar to the one you'd like to create via the Rescale API\n",
    "\n",
    "![](README.images/webportal_jobid.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs can have multiple analyses attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details of a job\n",
    "job_id = \"jNibFc\"\n",
    "results = requests.get(f\"{apiurl}/api/v2/jobs/{job_id}\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "# Display the analysis section\n",
    "pprint(results.json()['jobanalyses'][0]['analysis'])\n",
    "\n",
    "# Display the entire job definition JSON\n",
    "pprint(results.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `analyses` property is a list. This applies to multitile.\n",
    "\n",
    "Rescale API also allows to [list all user jobs](https://engineering.rescale.com/api-docs/#list-all-jobs). Let's count how many jobs completed this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = get_all_result_pages(\n",
    "    f\"{apiurl}/api/v2/jobs/\", headers, params={\"state\": \"completed\"}\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "this_year = datetime.today().year\n",
    "\n",
    "# dateInserted does not conform with ISO format - a small replacement is needed\n",
    "this_year_jobs = [\n",
    "    j\n",
    "    for j in jobs\n",
    "    if datetime.fromisoformat(j[\"dateInserted\"].replace(\"Z\", \"+00:00\")).year\n",
    "    == this_year\n",
    "]\n",
    "print(f\"Total jobs: {len(jobs)}, Jobs in {this_year}: {len(this_year_jobs)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grow our total jobs number and [create a new Job](https://engineering.rescale.com/api-docs/#create-a-job). We will use `miniconda` analysis to run a [Python script](job_inputs/calculate_pi.py) which estimates the value of π using the the [Leibniz’s formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80). The script requires an [input file](job_inputs/range.inp) which specifies the number of iterations. Estimated value is stored as text in `pi_estimate.res` and in a binary format in `pi_estimate.bin`.\n",
    "\n",
    "> NOTE: If your Workspace does not have the `Miniconda` software enabled, you can use any analysis as all Rescale clusters have a Python interpreter available. CHECK: Do we need a non-licensed software here?\n",
    "\n",
    "It may be the case, that your Rescale setup requires jobs to be submitted against a specific Project. Let's [list available projects](https://engineering.rescale.com/api-docs/#list-projects-available-to-your-user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = get_all_result_pages(f\"{apiurl}/api/v2/users/me/projects/\", headers)\n",
    "\n",
    "for p in projects: print(f\"{p['id']}\\t{p['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your projectId if required/desired\n",
    "project_id = None\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": \"python calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 1},\n",
    "        }\n",
    "    ],\n",
    "    \"projectId\": project_id\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{apiurl}/api/v2/jobs/\", headers=headers, json=job_definition)\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except:\n",
    "    pprint(response.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"hardware\"][\"coresPerSlot\"] = 2\n",
    "\n",
    "job_create_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_response.raise_for_status()\n",
    "\n",
    "pprint(job_create_response.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was successfuly created. We can check whether it is visible in the Rescale web portal.\n",
    "\n",
    "![](README.images/webportal_newjob.png)\n",
    "\n",
    "Job creation operation just saved the job, but it has not yet been submitted. In order to [submit a job](https://engineering.rescale.com/api-docs/#submit-a-saved-job) we need to capture its ID and call the `submit` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_create_response.json()[\"id\"]\n",
    "\n",
    "job_submit_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_submit_response.raise_for_status()\n",
    "\n",
    "print(f\"Status code: {job_submit_response.status_code}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status `200` signifies a success. If we go back to the web portal, we will job statuses changing.\n",
    "\n",
    "![](README.images/webportal_submittedjob.png)\n",
    "\n",
    "We want to wait until the job finishes. Let us poll for [job statuses](https://engineering.rescale.com/api-docs/#list-job-status-history) programatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_for_job_status(job_id, status, interval=30):\n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        job_status_response = get_all_result_pages(\n",
    "            f\"{apiurl}/api/v2/jobs/{job_id}/statuses/\", headers=headers\n",
    "        )\n",
    "        sorted_statuses = sorted(\n",
    "            job_status_response.json()[\"results\"],\n",
    "            key=lambda s: datetime.fromisoformat(s[\"statusDate\"]),\n",
    "        )\n",
    "        status_items = [s for s in sorted_statuses if s[\"status\"] == status]\n",
    "        print(\" > \".join([s[\"status\"] for s in sorted_statuses]))\n",
    "\n",
    "        if len(status_items) > 0:\n",
    "            return status_items[0]\n",
    "\n",
    "        time.sleep(interval)\n",
    "\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job went through all the statuses and reached the desired terminal state, however, it seems that the calculation failed. Let's try to figure out why by [listing output files](https://engineering.rescale.com/api-docs/#list-job-output-files). We will search for files that have `output` string in their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = get_all_result_pages(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": \"output\"}\n",
    ")\n",
    "\n",
    "pprint(output_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_output.log` file is created for all jobs and captures everything written to a console (standard output and error streams). Let's [fetch plaintext contents](https://engineering.rescale.com/api-docs/#get-plaintext-content-of-a-file) of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = output_files[0][\"id\"]\n",
    "\n",
    "file_contents_response = requests.get(\n",
    "    f\"{apiurl}/api/v2/files/{file_id}/lines\", headers=headers\n",
    ")\n",
    "file_contents_response.raise_for_status()\n",
    "\n",
    "for line in file_contents_response.json()[\"lines\"]:\n",
    "    print(line, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! All is clear. We forgot to upload input files. Let's [upload them](https://engineering.rescale.com/api-docs/#upload-a-file) and capture their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_upload(file_path):\n",
    "    import os\n",
    "\n",
    "    files = [\n",
    "        (\n",
    "            \"file\",\n",
    "            (\n",
    "                os.path.basename(file_path),\n",
    "                open(file_path, \"rb\"),\n",
    "                \"application/octet-stream\",\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    file_upload_response = requests.post(\n",
    "        f\"{apiurl}/api/v2/files/contents/\", headers=headers, files=files\n",
    "    )\n",
    "    file_upload_response.raise_for_status()\n",
    "\n",
    "    return file_upload_response.json()[\"id\"]\n",
    "\n",
    "\n",
    "file1_id = file_upload(\"job_inputs/calculate_pi.py\")\n",
    "file2_id = file_upload(\"job_inputs/range.inp\")\n",
    "\n",
    "print(file1_id, file2_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to update our previous job definition and resubmit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"inputFiles\"] = [{\"id\": file1_id}, {\"id\": file2_id}]\n",
    "pprint(job_definition)\n",
    "\n",
    "job_update_response = requests.patch(f\"{apiurl}/api/v2/jobs/{job_id}\", headers=headers, json=job_definition)\n",
    "job_update_response.raise_for_status()\n",
    "pprint(job_update_response.status_code)\n",
    "\n",
    "job_submit_response = requests.post(f\"{apiurl}/api/v2/jobs/{job_id}/submit/\", headers=headers)\n",
    "try:\n",
    "    job_submit_response.raise_for_status()\n",
    "except:\n",
    "    print(job_status_response.status_code)\n",
    "    print(job_status_response.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. Although the API responds with `200 OK` both of the above operations had no effect as chaning or re-submitting a Completed job is not allowed.\n",
    "\n",
    "> NOTE: This is going to be changed to return `400 BAD REQUEST` with an informative error message.\n",
    "\n",
    "Since we can not reuse our failed job (the public API does not support a clone operation) lets [delete it](https://engineering.rescale.com/api-docs/#delete-a-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_job_response = requests.delete(f\"{apiurl}/api/v2/jobs/{job_id}\", headers=headers)\n",
    "delete_job_response.raise_for_status()\n",
    "\n",
    "print(delete_job_response.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create new job with files attached, submit it and wait till it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (with inputs)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": \"python calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2},\n",
    "            \"inputFiles\": [{\"id\": file1_id}, {\"id\": file2_id}],\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id\n",
    "}\n",
    "\n",
    "job_create_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_response.raise_for_status()\n",
    "job_id = job_create_response.json()[\"id\"]\n",
    "\n",
    "job_create_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_response.raise_for_status()\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with downloading output files. Let's get [cluster status history](https://engineering.rescale.com/api-docs/#list-cluster-status-history-for-a-job) to check how cluster associated with the job transitioned between states.\n",
    "\n",
    "> TODO: Where is the cluster status information useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_statuses = get_all_result_pages(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/cluster_statuses/\", headers\n",
    ")\n",
    "\n",
    "sorted_statuses = sorted(\n",
    "    cluster_statuses,\n",
    "    key=lambda s: datetime.fromisoformat(s[\"statusDate\"].replace(\"Z\", \"+00:00\")),\n",
    ")\n",
    "for s in sorted_statuses:\n",
    "    print(f\"{s['statusDate']}\\t{s['status']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of output files and download them all.\n",
    "\n",
    "```\n",
    "INPUT_FILE = 1\n",
    "TEMPLATE_FILE = 2\n",
    "PARAM_FILE = 3\n",
    "SCRIPT = 4\n",
    "OUTPUT_FILE = 5\n",
    "CASE_FILE = 8\n",
    "TEMPORARY_FILE = 10\n",
    "CHECKPOINT_ARCHIVE = 11\n",
    "SNAPSHOT_FILE = 12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out_dir = \"job_outputs\"\n",
    "Path(out_dir).mkdir(exist_ok=True)\n",
    "\n",
    "files = get_all_result_pages(f\"{apiurl}/api/v2/jobs/{job_id}/files/\", headers)\n",
    "pprint(files)\n",
    "for f in files:\n",
    "    response = requests.get(\n",
    "        f\"{apiurl}/api/v2/files/{f['id']}/contents\", headers=headers)\n",
    "\n",
    "    chunk_size = 4096\n",
    "    with open(Path(out_dir, f[\"name\"]), 'wb') as fd:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can [retrive metatdata](https://engineering.rescale.com/api-docs/#get-metadata-of-a-file) for each file and compare checksums with downloaded files (yeah, we already fetched this all, just a demonstration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "for f in files:\n",
    "    file_metadata_response = requests.get(\n",
    "        f\"{apiurl}/api/v2/files/{f['id']}/\", headers=headers\n",
    "    )\n",
    "    file_metadata_response.raise_for_status()\n",
    "    remote_hash = file_metadata_response.json()\n",
    "\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "    chunk_size = 4096\n",
    "    \n",
    "    with open(Path(out_dir, f[\"name\"]), \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(chunk_size), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    if hash_sha512.hexdigest() == f[\"fileChecksums\"][0][\"fileHash\"]:\n",
    "        print(f\"OK\\t{f['name']}\")\n",
    "    else:\n",
    "        print(f\"FAIL\\t{f['name']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's clean up our job by deleting it together with input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_delete_response = requests.delete(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}\", headers=headers, params={\"deleteInputFiles\": True}\n",
    ")\n",
    "job_delete_response.raise_for_status()\n",
    "print(job_create_response.status_code)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel file download\n",
    "\n",
    "Rescale API file download endpoint supports [HTTP Range requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). This alows client code to split download into sever chunks and \n",
    "\n",
    "Let's start from submitting a job that will generate a 1GB result file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_NAME = \"testfile\"\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (1GB result file)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": f\"openssl rand -out ${TEST_FILE_NAME} -base64 792917038\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2},\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "}\n",
    "\n",
    "job_create_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_response.raise_for_status()\n",
    "job_id = job_create_response.json()[\"id\"]\n",
    "\n",
    "job_create_response = requests.post(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_response.raise_for_status()\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import threading\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "Chunk = namedtuple(\"Chunk\", [\"idx\", \"start_byte\", \"end_byte\"])\n",
    "FILE_IO_CHUNK_SIZE = 4096\n",
    "\n",
    "\n",
    "def _calculate_file_hash(file_path):\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(FILE_IO_CHUNK_SIZE), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    return hash_sha512.hexdigest()\n",
    "\n",
    "\n",
    "def _concatenate_files(output_file, *input_files, delete_chnks=True):\n",
    "    with open(output_file, \"wb\") as output:\n",
    "        for file_name in input_files:\n",
    "            with open(file_name, \"rb\") as input_file:\n",
    "                chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "                while chunk:\n",
    "                    output.write(chunk)\n",
    "                    chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "            if delete_chnks:\n",
    "                os.remove(file_name)\n",
    "\n",
    "\n",
    "def _chunk_download(apiurl, headers, file_id, dir_path, file_chunk: Chunk):\n",
    "    headers[\"Range\"] = f\"bytes={file_chunk.start_byte}-{file_chunk.end_byte}\"\n",
    "    response = requests.get(\n",
    "        f\"{apiurl}/api/v2/files/{file_id}/contents\", headers=headers\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(Path(dir_path, f\"{file_id}.{file_chunk.idx}\"), \"wb\") as fd:\n",
    "        for chunk in response.iter_content(FILE_IO_CHUNK_SIZE):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "def parallel_download(\n",
    "    apiurl,\n",
    "    headers,\n",
    "    file_id,\n",
    "    dir_path=\".\",\n",
    "    file_name=None,\n",
    "    num_threads=10,\n",
    "    threshold_mb=50,\n",
    "):\n",
    "    # get file size to determine whether splitting makes sense\n",
    "    response = requests.get(f\"{apiurl}/api/v2/files/{file_id}/\", headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    decrypted_size = response.json()[\"decryptedSize\"]\n",
    "\n",
    "    # Check if size is above threshhold, if not download in one piece\n",
    "    num_threads = num_threads if decrypted_size > threshold_mb * 1048576 else 1\n",
    "\n",
    "    file_hash = response.json()[\"fileChecksums\"][0][\"fileHash\"]\n",
    "    file_name = file_name if file_name != None else response.json()[\"name\"]\n",
    "\n",
    "    chunk_size = int(decrypted_size / num_threads)\n",
    "    threads = []\n",
    "    for idx in range(0, num_threads):\n",
    "        start_byte = idx * chunk_size\n",
    "\n",
    "        # Make sure the last chunk fetches all remaining bytes\n",
    "        end_byte = (\n",
    "            (idx + 1) * chunk_size - 1 if idx != num_threads - 1 else decrypted_size + 1\n",
    "        )\n",
    "\n",
    "        t = threading.Thread(\n",
    "            target=_chunk_download,\n",
    "            args=(\n",
    "                apiurl,\n",
    "                headers,\n",
    "                file_id,\n",
    "                dir_path,\n",
    "                Chunk(idx, start_byte, end_byte),\n",
    "            ),\n",
    "        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    _concatenate_files(\n",
    "        file_name, *[f\"{file_id}.{idx}\" for idx in range(0, num_threads)]\n",
    "    )\n",
    "\n",
    "    # Compare file hashes to make sure downloaded file is not corrupted.\n",
    "    filehash = _calculate_file_hash(file_name)\n",
    "    if filehash != file_hash:\n",
    "        raise Exception(\"File hashes not equal. Corrupted file download.\")\n",
    "\n",
    "\n",
    "# Get test file ID\n",
    "output_files_res = get_all_result_pages(\n",
    "    f\"{apiurl}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": TEST_FILE_NAME}\n",
    ")\n",
    "test_file_id = output_files_res[0][\"id\"]\n",
    "\n",
    "# Compare download speed\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "parallel_download(apiurl, headers, test_file_id, num_threads=10)\n",
    "print(f\"Time with 10 threads: {time.time() - t1}s\")\n",
    "\n",
    "t1 = time.time()\n",
    "parallel_download(apiurl, headers, test_file_id, num_threads=1)\n",
    "print(f\"Time with 1 thread: {time.time() - t1}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your connection speed and hardware we should see download time reduction, for example\n",
    "\n",
    "```\n",
    "Time with 10 threads: 35.551738023757935\n",
    "Time with 1 thread: 124.59398603439331\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage devices\n",
    "\n",
    "> TODO\n",
    "\n",
    "## Snapshots, Runs, Tasks and DOE Jobs\n",
    "\n",
    "> TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
