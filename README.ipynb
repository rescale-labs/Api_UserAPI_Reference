{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User API\n",
    "\n",
    "Full documentation of the Rescale API is available at [https://engineering.rescale.com/api-docs](https://engineering.rescale.com/api-docs). This document builds a story that aims to cover the entire User API surface.\n",
    "\n",
    "## Starting the notebook\n",
    "\n",
    "The only prerequisite to running this tutorial locally is a working installation of Python 3.8+. To obtain a local copy of the repository, clone it with `git` or [download zip](https://github.com/rescale-labs/Api_UserAPI_Reference/archive/refs/heads/main.zip). We're going to work within a virtual environment so that our dependencies are installed locally without affecting the global installation.\n",
    "\n",
    "```\n",
    "# Clone or unzip repository\n",
    "$ git clone https://github.com/rescale-labs/Api_UserAPI_Reference.git\n",
    "\n",
    "$ cd Api_UserAPI_Reference\n",
    "$ python -m venv .venv\n",
    "$ . .venv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "$ jupyter notebook README.ipynb\n",
    "```\n",
    "\n",
    "Jupyter Notebook cells need to be executed in order, as subsequent cells may depend on imports and variables defined in previous cells.\n",
    "\n",
    "### Python and JSON\n",
    "\n",
    "Python dictionaries can be mapped 1-1 to JSON format. The requests module automatically translates between a dictionary and the JSON format when sending payload and fetching results. The following code demonstrates this mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"string_property\": \"string\",\n",
    "    \"numeric_property\": 12,\n",
    "    \"array_property\": [{\"id\": 1}, {\"id\": 2}],\n",
    "    \"object_property\": {\n",
    "        \"name\": \"My Job\",\n",
    "        \"walltime\": 12,\n",
    "        \"interactive\": False,\n",
    "        \"files\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "print(json.dumps(payload, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorization\n",
    "\n",
    "The Rescale API authorizes requests using an *API Key*. This key is unique per User, per Workspace. A give user may have multiple *API Keys* active - one per Workspace.\n",
    "\n",
    "To generate an *API Key*, go to the API section of the User Profile.\n",
    "\n",
    "![](README.images/user_profile_apikey.png)\n",
    "\n",
    "> NOTE: Remember that your *API Key* is a secret. Never store it in a code repository. Never share it with your colleagues. If you realize that your *API Key* was exposed - delete it in the user profile and generate a new key.\n",
    "\n",
    "Rescale tools like Rescale CLI look for `apiconfig` authorization configuration file in users' home directory.\n",
    "\n",
    "```\n",
    "$HOME/.config/rescale/apiconfig             # Linux\n",
    "%USERPROFILE%\\config\\rescale\\apiconfig      # Windows\n",
    "```\n",
    "\n",
    "We will use this convention to store our credentials. Create the `apiconfig` text file in the aforementioned location and fill it with the following lines\n",
    "\n",
    "```\n",
    "[default]\n",
    "apibaseurl = https://eu.rescale.com\n",
    "apikey = 79a49b3132335a44742e86c9126e5cfaa1ea2489\n",
    "```\n",
    "\n",
    "If you need to execute your script on several platforms, you can define alternative profiles (for example `[us]`). The [config.py](config.py) module is provided for your convenience. You can copy it next to your scripts and use it as a standard way to retrieve credentials. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "api_url, api_key = config.get_profile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our `apikey` and a base URL for our API calls. Let's make the first call to get our user details. To make REST API calls, we will use the [`requests`](https://requests.readthedocs.io/en/latest/) module. It is listed as a dependency in the [`requirements.txt`](requirements.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "results = requests.get(f\"{api_url}/api/v2/users/me/\")\n",
    "if results.status_code != 200:\n",
    "    print(results.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got status code `401 Unauthorized` which suggests that we're missing our credentials. Let's define `headers` that we will use to authorize future API calls. Instead of checking for result code, let's use a function that raises an exception for all statuses that signify lack of success. Finally, we pretty-print the response JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "results = requests.get(f\"{api_url}/api/v2/users/me/\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(results.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to proceed and build up job submission.\n",
    "\n",
    "## Coretypes and Analyses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job definition requires us to specify software to be attached to a cluster running our job and the hardware (coretype and cores count). Let's [get the list of coretypes](https://engineering.rescale.com/api-docs/#coretypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coretypes_res = requests.get(f\"{api_url}/api/v2/coretypes/\", headers=headers)\n",
    "coretypes_res.raise_for_status()\n",
    "\n",
    "# Display the total count of coretypes and the amount present in the response\n",
    "print(\n",
    "    f\"Count: {coretypes_res.json()['count']}; Length: {len(coretypes_res.json()['results'])}\"\n",
    ")\n",
    "\n",
    "# Display reference to the next page\n",
    "print(f\"Next page: {coretypes_res.json()['next']}\")\n",
    "\n",
    "# Display a coretype object and its properties\n",
    "pprint(coretypes_res.json()[\"results\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that the response returned a total count of coretypes that is larger than the total count of coretype objects in the `results` list. This will often happen for endpoints that potentially return large amounts of data. In such situations, results are paged, and each page contains a reference to the `next` page.\n",
    "\n",
    "Since paging is common, let's define a function that will loop through all the pages and aggregate objects returned in the `results` list of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_result_pages(url, headers={}, params={}):\n",
    "    results = []\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    results.extend(res.json()[\"results\"])\n",
    "\n",
    "    while res.json()[\"next\"] != None:\n",
    "        res = requests.get(res.json()[\"next\"], headers=headers)\n",
    "        res.raise_for_status()\n",
    "        results.extend(res.json()[\"results\"])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coretypes = get_all_result_pages(f\"{api_url}/api/v2/coretypes/\", headers)\n",
    "\n",
    "print(f\"Length: {len(coretypes)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now confident that we fetched all coretypes. For our simple job, we will need a general purpose coretype with a low corecount. Let's list coretype `code`s in the `general` category together with their `cores` counts and `processorInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_coretypes = {\n",
    "    c[\"code\"]: {\"cores\": c[\"cores\"], \"processorInfo\": c[\"processorInfo\"]}\n",
    "    for c in coretypes\n",
    "    if \"general\" in c[\"categoryCodes\"]\n",
    "}\n",
    "\n",
    "pprint(general_coretypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a shortlist, we need to decide which coretype to use. Since our test calculation does not have specific requirements, we will use the least expensive option. Let's [fetch coretype prices](https://engineering.rescale.com/api-docs/#list-all-compute-prices), filter prices for on-demand economy (ODE) an `linux` OS, link them with general purpose coretypes and get the minimum price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prices\n",
    "prices_res = requests.get(f\"{api_url}/api/v2/billing/computeprices/\", headers=headers)\n",
    "prices_res.raise_for_status()\n",
    "\n",
    "# Print sample pricing object\n",
    "pprint(prices_res.json()[-1])\n",
    "\n",
    "# Filter out active ODE linux based coretypes\n",
    "linux_ode_coretypes = {\n",
    "    c[\"coreType\"]: float(c[\"value\"][\"amount\"])\n",
    "    for c in prices_res.json()\n",
    "    if c[\"planType\"].endswith(\"on-demand\")\n",
    "    and c[\"os\"] == \"linux\"\n",
    "    and c[\"isActive\"]\n",
    "    and c[\"coreType\"] in general_coretypes\n",
    "}\n",
    "\n",
    "# Find the least expensive one\n",
    "min_price_coretype = min(linux_ode_coretypes, key=linux_ode_coretypes.get)\n",
    "print(\n",
    "    f\"Lowest price coretype: {min_price_coretype} at {linux_ode_coretypes[min_price_coretype]}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot market prices are dynamic and therefore your cheapest coretype may change even during a day. Let's assume that `granite` is the coretype we want to use.\n",
    "\n",
    "We know which coretype to use, we still need to select the analysis. To build our job creation request, we need to know analysis and version codes. Let's query API for the [list of available analyses](https://engineering.rescale.com/api-docs/#analyses) (software tiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses = get_all_result_pages(f\"{api_url}/api/v2/analyses/\", headers)\n",
    "\n",
    "versions_count = 0\n",
    "for a in analyses:\n",
    "    versions_count += len(a[\"versions\"])\n",
    "\n",
    "print(\n",
    "    f\"Total number of analyses: {len(analyses)}, Total number of versions: {versions_count}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample analysis code: {analyses[0]['code']} and version code: {analyses[0]['versions'][0]['versionCode']}\"\n",
    ")\n",
    "\n",
    "pprint(analyses[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your workspace does not have software filters in place, you should see over 700 analyses and over 3200 unique versions. It may be cumbersome to find the code and version that you need. A more practical approach is to create a job using the web portal and then query the Rescale API to get a JSON document listing job properties (the Job resource model). We will do it in the next section.\n",
    "\n",
    "## Jobs, Files and Clusters\n",
    "\n",
    "To fetch the definition of a previously saved Job, go to the portal and get a Job ID for a job similar to the one you'd like to create via the Rescale API.\n",
    "\n",
    "![](README.images/webportal_jobid.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we captured the Job ID, we can [get a specific Job](https://engineering.rescale.com/api-docs/#get-a-specific-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to match ID of your job\n",
    "job_id = \"ObMcJc\"\n",
    "\n",
    "results = requests.get(f\"{api_url}/api/v2/jobs/{job_id}\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "# Display the analysis section\n",
    "pprint(results.json()[\"jobanalyses\"][0][\"analysis\"])\n",
    "\n",
    "# Display the entire job definition JSON\n",
    "pprint(results.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `analyses` property is a list. This represents a possibility of having multiple software tiles (analyses) attached to a single Job.\n",
    "\n",
    "The Rescale API also allows to [list all user jobs](https://engineering.rescale.com/api-docs/#list-all-jobs). Let's count how many jobs completed this year. Additional query parameters allow filtering jobs by:\n",
    "\n",
    "* `state` - one of `(completed, not_completed)`\n",
    "* `job_status` - one of `(PENDING, QUEUED, STARTED, VALIDATED, EXECUTING, COMPLETED, STOPPING, WAITING_FOR_CLUSTER, FORCE_STOP, WAITING_FOR_QUEUE)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/\",\n",
    "    headers,\n",
    "    params={\"state\": \"completed\", \"job_status\": \"COMPLETED\"},\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "this_year = datetime.today().year\n",
    "\n",
    "# dateInserted does not conform with ISO format - a small replacement is needed\n",
    "this_year_jobs = [\n",
    "    j\n",
    "    for j in jobs\n",
    "    if datetime.fromisoformat(j[\"dateInserted\"].replace(\"Z\", \"+00:00\")).year\n",
    "    == this_year\n",
    "]\n",
    "print(f\"Total jobs: {len(jobs)}, Jobs in {this_year}: {len(this_year_jobs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow by creating a new Job. We will use `user_included` analysis (Bring Your Own Software) to run a [Python script](job_inputs/calculate_pi.py) which estimates the value of π using the [Leibniz’s formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80). The script requires an [input file](job_inputs/range.inp) which specifies the number of iterations. Estimated value is stored as text in `pi_estimate.res` and in a binary format in `pi_estimate.bin`.\n",
    "\n",
    "> NOTE: If your Workspace does not have the Bring Your Own Software software tile enabled, you can use any analysis as all Rescale clusters have a Python interpreter available.\n",
    "\n",
    "It may be the case that your Rescale setup requires jobs to be submitted against a specific Project. Let's [list available projects](https://engineering.rescale.com/api-docs/#list-projects-available-to-your-user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = get_all_result_pages(f\"{api_url}/api/v2/users/me/projects/\", headers)\n",
    "\n",
    "for p in projects:\n",
    "    print(f\"{p['id']}\\t{p['name']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the needed information about the hardware (`coreType: granite`) and software (`code: user_included; version: 0`) we can specify our job definition payload and [create a Job](https://engineering.rescale.com/api-docs/#create-a-job). To keep things simple in this tutorial, we're not handling exceptions raised by our HTTP requests.\n",
    "\n",
    "Production code, should try to recover from an exception or give end user information on how to proceed. Here, we expect an exception, so we will catch it and display an error. This demonstrates the Rescale API returning useful error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your projectId if required/desired\n",
    "project_id = None\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": \"python3 calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 1, \"walltime\": 1},\n",
    "        }\n",
    "    ],\n",
    "    \"projectId\": project_id,\n",
    "}\n",
    "\n",
    "job_save_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "try:\n",
    "    job_save_res.raise_for_status()\n",
    "except:\n",
    "    pprint(job_save_res.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that the coretype we have selected, `granite`, does not support configuration with `1` core per slot. Let's fix it by updating the Job definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"hardware\"][\"coresPerSlot\"] = 2\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "pprint(job_create_res.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was successfully created. We can check whether it is visible in the Rescale web portal.\n",
    "\n",
    "![](README.images/webportal_newjob.png)\n",
    "\n",
    "Job creation operation just saved the job, it has not yet been submitted. To [submit a job](https://engineering.rescale.com/api-docs/#submit-a-saved-job), we need to capture its ID and call the `submit` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_submit_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_submit_res.raise_for_status()\n",
    "\n",
    "print(f\"Status code: {job_submit_res.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status `200` signifies a success. If we go back to the web portal, we will see job statuses changing.\n",
    "\n",
    "![](README.images/webportal_submittedjob.png)\n",
    "\n",
    "We want to wait until the job finishes. Let us poll for [job statuses](https://engineering.rescale.com/api-docs/#list-job-status-history) programmatically. We will define a utility function that polls until a specific Job status is reached and prints out statuses on every poll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_for_job_status(\n",
    "    api_url, headers, job_id, poll_status, interval=30, verbose=False\n",
    "):\n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        job_statuses = get_all_result_pages(\n",
    "            f\"{api_url}/api/v2/jobs/{job_id}/statuses/\", headers=headers\n",
    "        )\n",
    "        if verbose:\n",
    "            sorted_statuses = sorted(\n",
    "                job_statuses,\n",
    "                key=lambda s: datetime.fromisoformat(s[\"statusDate\"]),\n",
    "            )\n",
    "            print(\" > \".join([s[\"status\"] for s in sorted_statuses]))\n",
    "\n",
    "        status_items = [s for s in sorted_statuses if s[\"status\"] == poll_status]\n",
    "        if len(status_items) > 0:\n",
    "            return status_items[0]\n",
    "\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_for_job_status(api_url, headers, job_id, \"Completed\", verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job went through all the statuses and reached the desired terminal state, however, it seems that the calculation failed. Let's try to figure out why by [listing output files](https://engineering.rescale.com/api-docs/#list-job-output-files). We will search for files that have `output` string in their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": \"output\"}\n",
    ")\n",
    "\n",
    "pprint(output_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_output.log` file is created for all jobs and captures everything written to a console (standard output and error streams). Let's [fetch the plaintext contents](https://engineering.rescale.com/api-docs/#get-plaintext-content-of-a-file) of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = output_files[0][\"id\"]\n",
    "\n",
    "file_contents_response = requests.get(\n",
    "    f\"{api_url}/api/v2/files/{file_id}/lines/\", headers=headers\n",
    ")\n",
    "file_contents_response.raise_for_status()\n",
    "\n",
    "for line in file_contents_response.json()[\"lines\"]:\n",
    "    print(line, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! All is clear. We forgot to upload input files. Let's [upload files](https://engineering.rescale.com/api-docs/#upload-a-file) and capture their IDs. Since file upload is a frequent operation, we will encapsulate in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def file_upload(api_url, headers, file_path):\n",
    "    files = [\n",
    "        (\n",
    "            \"file\",\n",
    "            (\n",
    "                os.path.basename(file_path),\n",
    "                open(file_path, \"rb\"),\n",
    "                \"application/octet-stream\",\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    file_upload_res = requests.post(\n",
    "        f\"{api_url}/api/v2/files/contents/\", headers=headers, files=files\n",
    "    )\n",
    "    file_upload_res.raise_for_status()\n",
    "\n",
    "    return file_upload_res.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_id = file_upload(api_url, headers, \"job_inputs/calculate_pi.py\")\n",
    "file2_id = file_upload(api_url, headers, \"job_inputs/range.inp\")\n",
    "\n",
    "print(file1_id, file2_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to update our previous job definition by extending the job analysis specification with the `inputFiles` property, and resubmit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"inputFiles\"] = [{\"id\": file1_id}, {\"id\": file2_id}]\n",
    "pprint(job_definition)\n",
    "\n",
    "job_update_res = requests.patch(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_update_res.raise_for_status()\n",
    "pprint(job_update_res.status_code)\n",
    "\n",
    "job_submit_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "try:\n",
    "    job_submit_res.raise_for_status()\n",
    "except:\n",
    "    print(job_submit_res.status_code)\n",
    "    print(job_submit_res.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. Although the API responds with `200 OK` for `PATCH`, both of the above operations had no effect, as changing or re-submitting a Completed job is not allowed.\n",
    "\n",
    "> NOTE: This is going to be changed to return `400 BAD REQUEST` with an informative error message.\n",
    "\n",
    "Since we cannot reuse our failed job (the Rescale API does not support a clone operation) let's [delete it](https://engineering.rescale.com/api-docs/#delete-a-job).\n",
    "\n",
    "Before we call a `DELETE` operation on a Job, we need to make sure that the underlying cluster of the job reached its `Stopped` state. If we try to delete a `Completed` job with Job's cluster still running, the API will return `400 BAD REQUEST`. Let's define a function to poll for either job status or for [cluster status](https://engineering.rescale.com/api-docs/#list-cluster-status-history-for-a-job). The code will wait until the cluster reaches the `\"Stopped\"` state and then delete a Job.\n",
    "\n",
    "Cluster statuses can have the following values: `(\"Stopped\", \"Stopping\", \"Stop Requested\", \"Started\", \"Starting\", \"Not Started\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class JobStatus(Enum):\n",
    "    PENDING = \"Pending\"\n",
    "    QUEUED = \"Queued\"\n",
    "    STARTED = \"Started\"\n",
    "    VALIDATED = \"Validated\"\n",
    "    EXECUTING = \"Executing\"\n",
    "    COMPLETED = \"Completed\"\n",
    "    STOPPING = \"Stopping\"\n",
    "    WAITING_FOR_CLUSTER = \"Waiting for Cluster\"\n",
    "    FORCE_STOP = \"Force Stop\"\n",
    "    WAITING_FOR_QUEUE = \"Waiting for Queue\"\n",
    "\n",
    "\n",
    "class ClusterStatus(Enum):\n",
    "    NOT_STARTED = \"Not Started\"\n",
    "    PENDING = \"Pending\"\n",
    "    QUEUED = \"Queued\"\n",
    "    STARTING = \"Starting\"\n",
    "    STARTED = \"Started\"\n",
    "    STOP_REQUESTED = \"Stop Requested\"\n",
    "    STOPPING = \"Stopping\"\n",
    "    STOPPED = \"Stopped\"\n",
    "    UNKNOWN = \"Unknown\"\n",
    "    ERROR = \"Error\"\n",
    "\n",
    "\n",
    "def poll_for_status(api_url, headers, job_id, status, interval=30, verbose=False):\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    while True:\n",
    "        if type(status) == type(JobStatus.COMPLETED):\n",
    "            statuses = get_all_result_pages(\n",
    "                f\"{api_url}/api/v2/jobs/{job_id}/statuses/\", headers=headers\n",
    "            )\n",
    "        elif type(status) == type(ClusterStatus.STARTED):\n",
    "            statuses = get_all_result_pages(\n",
    "                f\"{api_url}/api/v2/jobs/{job_id}/cluster_statuses/\", headers=headers\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Unknown Status Type.\")\n",
    "\n",
    "        if verbose:\n",
    "            sorted_statuses = sorted(\n",
    "                statuses,\n",
    "                # Currently dates are returned with inconsistent formatting\n",
    "                key=lambda s: datetime.fromisoformat(s[\"statusDate\"])\n",
    "                if type(status) == type(JobStatus.COMPLETED)\n",
    "                else datetime.fromisoformat(s[\"statusDate\"].replace(\"Z\", \"+00:00\")),\n",
    "            )\n",
    "            print(\" > \".join([s[\"status\"] for s in sorted_statuses]))\n",
    "\n",
    "        status_items = [s for s in sorted_statuses if s[\"status\"] == status.value]\n",
    "        if len(status_items) > 0:\n",
    "            return status_items[0]\n",
    "\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_for_status(api_url, headers, job_id, ClusterStatus.STOPPED, verbose=True)\n",
    "\n",
    "delete_job_res = requests.delete(f\"{api_url}/api/v2/jobs/{job_id}/\", headers=headers)\n",
    "delete_job_res.raise_for_status()\n",
    "\n",
    "print(delete_job_res.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are to create a new job with files attached, submit it and wait until it completes. With the new Job definition, we also specify that we want to run the job using On-Demand Economy (spot market) instances (the `isLowPriority` property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (with inputs)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": \"python3 calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2, \"walltime\": 1},\n",
    "            \"inputFiles\": [{\"id\": file1_id}, {\"id\": file2_id}],\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "    \"isLowPriority\": True,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_status(api_url, headers, job_id, JobStatus.COMPLETED, verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of output files and download them all. Downloading files is a frequent operation, let's encapsulate it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_file(api_url, headers, file_id, out_dir, file_name):\n",
    "    response = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{file_id}/contents/\", headers=headers\n",
    "    )\n",
    "\n",
    "    chunk_size = 4096\n",
    "    with open(Path(out_dir, file_name), \"wb\") as fd:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"job_outputs\"\n",
    "Path(out_dir).mkdir(exist_ok=True)\n",
    "\n",
    "files = get_all_result_pages(f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers)\n",
    "pprint(files)\n",
    "\n",
    "for f in files:\n",
    "    download_file(api_url, headers, f[\"id\"], out_dir, f[\"name\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale Files metadata contains the `typeId` property, which can have one of the following values.\n",
    "\n",
    "| id | type                 |\n",
    "| -- | -------------------- |\n",
    "| 1  | INPUT_FILE           |\n",
    "| 2  | TEMPLATE_FILE        |\n",
    "| 3  | PARAM_FILE           |\n",
    "| 4  | SCRIPT               |\n",
    "| 5  | OUTPUT_FILE          |\n",
    "| 8  | CASE_FILE            |\n",
    "| 10 | TEMPORARY_FILE       |\n",
    "| 11 | CHECKPOINT_ARCHIVE   |\n",
    "| 12 | SNAPSHOT_FILE        |\n",
    "\n",
    "We can [retrieve metadata](https://engineering.rescale.com/api-docs/#get-metadata-of-a-file) for each file and compare checksums with downloaded files. We fetched this information already when listing job files, here we demonstrate using the `/files` resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "for f in files:\n",
    "    file_metadata_res = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{f['id']}/\", headers=headers\n",
    "    )\n",
    "    file_metadata_res.raise_for_status()\n",
    "    remote_hash = file_metadata_res.json()\n",
    "\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "    chunk_size = 4096\n",
    "\n",
    "    with open(Path(out_dir, f[\"name\"]), \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(chunk_size), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    if hash_sha512.hexdigest() == f[\"fileChecksums\"][0][\"fileHash\"]:\n",
    "        print(f\"OK\\t{f['name']}\")\n",
    "    else:\n",
    "        print(f\"FAIL\\t{f['name']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up our job by deleting it together with input files (remember to wait for cluster to be stopped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_for_status(api_url, headers, job_id, ClusterStatus.STOPPED, verbose=True)\n",
    "\n",
    "job_delete_res = requests.delete(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/\",\n",
    "    headers=headers,\n",
    "    params={\"deleteInputFiles\": True},\n",
    ")\n",
    "job_delete_res.raise_for_status()\n",
    "print(job_create_res.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License settings and complex Job definitions\n",
    "\n",
    "It is possible to attach several analyses (software tiles) to a single Job. Commands of such analyses are executed in order they have been specifies in the `jobanalyses` array (or left-to-right in the Rescale Portal). This is usefeul in cases where we have coupled workflows, for example couplicng mechnical and fluid calculations.\n",
    "\n",
    "There will be situations where multiple softwares are attached to a Job, but there is a single script that drives the calculation. In such cases only one `command` property can be filled. It does not matter which one. To pass command validation performed by the platform, specify an empty command as `\"command\": \"# \"`.\n",
    "\n",
    "License server settings are passed as environmental variables. In the example below ANSYS license settings are passed as `ANSYSLI_SERVERS` and `ANSYSLMD_LICENSE_FILE` environemntal variables. To find out which environmental variables need to be set, it is best to set up the anaysis manually in the Rescale Portal, save it and the fetch it via the API (as in the beggining of the [Jobs, Files and Clusters](#jobs-files-and-clusters) section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_id = file_upload(api_url, headers, \"job_inputs/calculate_pi.py\")\n",
    "file2_id = file_upload(api_url, headers, \"job_inputs/range.inp\")\n",
    "file3_id = file_upload(api_url, headers, \"job_inputs/run_plot.jou\")\n",
    "file4_id = file_upload(api_url, headers, \"job_inputs/tjunction_plot.cas.gz\")\n",
    "\n",
    "job_definition = {\n",
    "    \"isLowPriority\": False,\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (multi tile job with licensed software)\",\n",
    "    \"description\": \"\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": \"python3 calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coresPerSlot\": 4, \"coreType\": \"emerald_max\", \"walltime\": 1},\n",
    "            \"inputFiles\": [\n",
    "                {\"id\": file1_id},\n",
    "                {\"id\": file2_id},\n",
    "                {\"id\": file3_id},\n",
    "                {\"id\": file4_id},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"envVars\": {\n",
    "                \"ANSYSLI_SERVERS\": \"2325@ansys.server.com\",\n",
    "                \"ANSYSLMD_LICENSE_FILE\": \"1055@ansys.server.com\",\n",
    "            },\n",
    "            \"analysis\": {\"code\": \"ansys_fluent\", \"version\": \"2019r1\"},\n",
    "            \"command\": \"fluent 3ddp -gu -ssh -cnf=$FLUENT_HOSTS -t$RESCALE_CORES_PER_SLOT -i run_plot.jou -driver x11\",\n",
    "            \"hardware\": {\"coresPerSlot\": 4, \"coreType\": \"emerald_max\", \"walltime\": 1},\n",
    "        },\n",
    "    ],\n",
    "    \"projectId\": None,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go to the Rescale Portal and inspect our Job definition visually. It should look similar to the following recording. If you do have a Fluent license, you can proceed with submitting the Job.\n",
    "\n",
    "![](README.images/multi_tile_job_with_license.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi persistent clusters\n",
    "\n",
    "There are simulation workflows where execution is driven by some external control process. For example, an optimizer, that will submit multiple calculations with slightly changed parameters to minimize some objective function.\n",
    "\n",
    "Instead of launching separate jobs, that start multiple clusters executing a predefined command, we could start a cluster and then execute commands from an external process, dynamically. If we execute consecutive jobs serially, using a quasi-persistent instead of submitting individual jobs, saves us time waiting for cluster startup. There are other possibilities like implementing fine-grained checkpointing when running on economy priority coretypes.\n",
    "\n",
    "[SSH](https://en.wikipedia.org/wiki/Secure_Shell) is used for both: executing commands on a remote cluster and transferring files (via [SCP](https://en.wikipedia.org/wiki/Secure_copy_protocol)). To enable SSH connections to Rescale clusters, a public key of a key-pair need to be registered in the User Profile. To generate keypair execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh-keygen -f rescale-key -P \"\"\n",
    "! cat rescale-key.pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed, copy the contents of the public key (`rescale-key.pub`) and paste it the Job Settings section of the User Profile as demonstrated below. Make sure to keep your private key secret. We will use it to authenticate with the cluster.\n",
    "\n",
    "![](README.images/user_profile_public_key.gif)\n",
    "\n",
    "To create a cluster that will keep running until it is stopped or a walltime is reached, we need to specify a command that will never exit. For example `\"command\": \"sleep inf\"`. Once a cluster is created (Job has a `EXECUTING` status), we can query the (undocumented) `/api/v2/jobs/{job_id}/instances/` API endpoint, to get cluster public IP address, username and SSH port. We will use this information to establish an SSH connection. Let's create a quasi-persistent cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = None\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (sleep inf)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": \"sleep inf\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2, \"walltime\": 1},\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "    \"isLowPriority\": True,\n",
    "}\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_status(api_url, headers, job_id, JobStatus.EXECUTING, verbose=True)\n",
    "\n",
    "instances = get_all_result_pages(f\"{api_url}/api/v2/jobs/{job_id}/instances/\", headers)\n",
    "pprint(instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract `username`, `sshPort` and `publicIp` properties of the cluster instance model. The `role` attribute is useful when we create multi-node clusters. We use it to distinguish between the head and worker nodes cluster instances will either have a role `MPI_MASTER` (head node) or `MPI_SLAVE` (for a worker node). Please excuse our language here.\n",
    "\n",
    "We will use the [Paramiko](https://www.paramiko.org/) python module to establish secure connection with the cluster. The [scp](https://github.com/jbardin/scp.py) python module uses a Paramiko transport to send and receive files. Let's try to estimate PI using quasi-persistent cluster. Here, instead of upload files as Job inputs, we copy these to a running cluster using SCP. Then we execute the command over SSH. See code comments for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import scp\n",
    "\n",
    "\n",
    "def key_based_connect(host, port, username, pkey_path):\n",
    "    pkey = paramiko.RSAKey.from_private_key_file(pkey_path)\n",
    "    client = paramiko.SSHClient()\n",
    "    policy = paramiko.AutoAddPolicy()\n",
    "    client.set_missing_host_key_policy(policy)\n",
    "    client.connect(host, port, username, pkey=pkey)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to private key\n",
    "PKEY_PATH = \"rescale-key\"\n",
    "\n",
    "public_ip = instances[0][\"publicIp\"]\n",
    "ssh_port = instances[0][\"sshPort\"]\n",
    "username = instances[0][\"username\"]\n",
    "\n",
    "print(f\"Connecting to {username}@{public_ip}:{ssh_port}\")\n",
    "\n",
    "ssh_client = key_based_connect(public_ip, ssh_port, username, PKEY_PATH)\n",
    "scp_client = scp.SCPClient(ssh_client.get_transport())\n",
    "\n",
    "scp_client.put(\"job_inputs/calculate_pi.py\", \"work/calculate_pi.py\")\n",
    "scp_client.put(\"job_inputs/range.inp\", \"work/range.inp\")\n",
    "\n",
    "# The exec_command returns immediately, so no result file will yet be available.\n",
    "# A direct way of waiting util command finishes is to keep reading the standard output.\n",
    "_, stdout, _ = ssh_client.exec_command(\"cd work && python3 calculate_pi.py range.inp\")\n",
    "for line in iter(stdout.readline, \"\"):\n",
    "    print(line, end=\"\")\n",
    "\n",
    "# Download and display the result file\n",
    "scp_client.get(\"work/pi_estimate.res\", \"job_outputs/pi_estimate.res_ssh.log\")\n",
    "with open(\"job_outputs/pi_estimate.res_ssh.log\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation is now completed, but the cluster is still running, spinning our our infinite sleep command (`sleep inf`). We need to explicitly shut it down. There are 3 ways to [stop a job](https://engineering.rescale.com/api-docs/#stop-a-job). In our case we want the Job to shutdown as we're not interested in syncing files to cloud storage. Let's shut it down (we expect response status code to be `202`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_stop_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/stop/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_stop_res.raise_for_status()\n",
    "print(job_stop_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stopping jobs executing complex commands, or when we want to sync files in the `$HOME/work` directory of our quasi-persistent cluster, then we need to stop our Job gracefully. We have two options:\n",
    "\n",
    "* Graceful stop, where `SIGTERM` signal sent to the `command` process with files synced to cloud storage (`/jobs/{job_id}/stop/`)\n",
    "* Forced stop, where a `SIGTERM` signal sent to the `command` process and then files are saved to cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing file transfers\n",
    "\n",
    "There are three main approaches to speeding up file transfers:\n",
    "\n",
    "* file size reduction through compression\n",
    "* transferring multiple files at the same time\n",
    "* splitting a file into chunks and transferring these concurrently\n",
    "\n",
    "The next sections will focus on compression and chunked transfers. Presented techniques of concurrent execution should be easy to apply to uploading multiple files at the same time. \n",
    "\n",
    "### Archiving files before upload\n",
    "\n",
    "The simplest approach to file transfer optimization is to use compression. The following script uses the standard Python `zipfile` module to create an archive of input files.\n",
    "\n",
    "Note that the `command` in the Job definition starts with changing the current directory to the directory we have compressed. Zip archives are automatically decompressed by the platform. Automatic decompression can be disabled by specifying an additional parameter to the file specification dictionary.\n",
    "\n",
    "```\n",
    "\"inputFiles\": [{\"id\": file_id, \"decompress\": False}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "ZIP_FILE = \"job_inputs.zip\"\n",
    "\n",
    "\n",
    "def create_zip(dir_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(file_path, dir_path)\n",
    "                zipf.write(file_path, rel_path)\n",
    "\n",
    "\n",
    "create_zip(\"job_inputs\", ZIP_FILE)\n",
    "file_id = file_upload(api_url, headers, ZIP_FILE)\n",
    "os.remove(ZIP_FILE)\n",
    "\n",
    "\n",
    "def get_process_output_lines(api_url, headers, job_id):\n",
    "    output_files = get_all_result_pages(\n",
    "        f\"{api_url}/api/v2/jobs/{job_id}/files/\",\n",
    "        headers=headers,\n",
    "        params={\"search\": \"process_output.log\"},\n",
    "    )\n",
    "    file_id = output_files[0][\"id\"]\n",
    "\n",
    "    file_contents_response = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{file_id}/lines\", headers=headers\n",
    "    )\n",
    "    file_contents_response.raise_for_status()\n",
    "\n",
    "    return file_contents_response.json()[\"lines\"]\n",
    "\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (with zipped inputs)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "\n",
    "            # Note that we need to cd into a directory\n",
    "            \"command\": \"cd job_inputs; python calculate_pi.py range.inp\",\n",
    "\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2, \"walltime\": 1},\n",
    "            \"inputFiles\": [{\"id\": file_id}],\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "    \"isLowPriority\": True,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_job_status(api_url, headers, job_id, \"Completed\", verbose=True)\n",
    "\n",
    "for l in get_process_output_lines(api_url, headers, job_id):\n",
    "    print(l, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent file download\n",
    "\n",
    "Rescale API file download endpoint supports [HTTP Range requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). This allows client code to split the download into several chunks and download these concurrently.\n",
    "\n",
    "Let's start from submitting a job that will generate a 1 GB result file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_NAME = \"testfile\"\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (1GB result file)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": f\"openssl rand -out {TEST_FILE_NAME} -base64 792917038\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2, \"walltime\": 1},\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_status(api_url, headers, job_id, \"Completed\", verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command of the above Job used `openssl` to create a random file that is now stored in cloud storage. The code below defines several functions that work together to download a file in chucks, each in its own thread. Consult the code comments for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import threading\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "Chunk = namedtuple(\"Chunk\", [\"idx\", \"start_byte\", \"end_byte\"])\n",
    "FILE_IO_CHUNK_SIZE = 4096\n",
    "\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(FILE_IO_CHUNK_SIZE), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    return hash_sha512.hexdigest()\n",
    "\n",
    "\n",
    "def concatenate_files(output_file, *input_files, delete_inputs=True):\n",
    "    with open(output_file, \"wb\") as output:\n",
    "        for file_name in input_files:\n",
    "            with open(file_name, \"rb\") as input_file:\n",
    "                chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "                while chunk:\n",
    "                    output.write(chunk)\n",
    "                    chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "            if delete_inputs:\n",
    "                os.remove(file_name)\n",
    "\n",
    "\n",
    "def _chunk_download(api_url, headers, file_id, dir_path, file_chunk: Chunk):\n",
    "    headers[\"Range\"] = f\"bytes={file_chunk.start_byte}-{file_chunk.end_byte}\"\n",
    "    response = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{file_id}/contents\", headers=headers\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(Path(dir_path, f\"{file_id}.{file_chunk.idx}\"), \"wb\") as fd:\n",
    "        for chunk in response.iter_content(FILE_IO_CHUNK_SIZE):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "def concurrent_download(\n",
    "    api_url,\n",
    "    headers,\n",
    "    file_id,\n",
    "    dir_path=\".\",\n",
    "    file_name=None,\n",
    "    num_threads=10,\n",
    "    threshold_mb=50,\n",
    "):\n",
    "    # get file size to determine whether splitting makes sense\n",
    "    response = requests.get(f\"{api_url}/api/v2/files/{file_id}/\", headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    decrypted_size = response.json()[\"decryptedSize\"]\n",
    "\n",
    "    # Check if size is above threshold, if not download in one piece\n",
    "    num_threads = num_threads if decrypted_size > threshold_mb * 1048576 else 1\n",
    "\n",
    "    file_hash = response.json()[\"fileChecksums\"][0][\"fileHash\"]\n",
    "    file_name = file_name if file_name != None else response.json()[\"name\"]\n",
    "\n",
    "    chunk_size = int(decrypted_size / num_threads)\n",
    "    threads = []\n",
    "    for idx in range(0, num_threads):\n",
    "        start_byte = idx * chunk_size\n",
    "\n",
    "        # Make sure the last chunk fetches all remaining bytes\n",
    "        end_byte = (\n",
    "            (idx + 1) * chunk_size - 1 if idx != num_threads - 1 else decrypted_size + 1\n",
    "        )\n",
    "\n",
    "        t = threading.Thread(\n",
    "            target=_chunk_download,\n",
    "            args=(\n",
    "                api_url,\n",
    "                headers,\n",
    "                file_id,\n",
    "                dir_path,\n",
    "                Chunk(idx, start_byte, end_byte),\n",
    "            ),\n",
    "        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    concatenate_files(\n",
    "        file_name, *[f\"{file_id}.{idx}\" for idx in range(0, num_threads)]\n",
    "    )\n",
    "\n",
    "    # Compare file hashes to make sure downloaded file is not corrupted.\n",
    "    filehash = calculate_file_hash(file_name)\n",
    "    if filehash != file_hash:\n",
    "        raise Exception(\"File hashes not equal. Corrupted file download.\")\n",
    "\n",
    "\n",
    "# Get test file ID\n",
    "output_files_res = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": TEST_FILE_NAME}\n",
    ")\n",
    "test_file_id = output_files_res[0][\"id\"]\n",
    "\n",
    "# Compare download speed\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "concurrent_download(api_url, headers, test_file_id, num_threads=10)\n",
    "print(f\"Time with 10 threads: {time.time() - t1} s\")\n",
    "\n",
    "t1 = time.time()\n",
    "concurrent_download(api_url, headers, test_file_id, num_threads=1)\n",
    "print(f\"Time with 1 thread: {time.time() - t1} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your connection speed and hardware, we should see download time reduction, for example\n",
    "\n",
    "```\n",
    "Time with 10 threads: 35.551738023757935 s\n",
    "Time with 1 thread: 124.59398603439331 s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent upload with archive volumes\n",
    "\n",
    "The file upload endpoint does not support range requests, a feature we used for concurrent downloads. It is, however, still possible to use the file splitting approach to speed up file uploads. The solution is to split a file on a local computer, upload parts, and reassemble the file within a Job.\n",
    "\n",
    "In our approach, we will use a compression utility that supports splitting an archive into volumes. This will give us an added benefit of reducing the number of bytes to transfer.\n",
    "\n",
    "The default `7zip` tool available on Rescale clusters does not support the latest, high-throughput compression algorithms. Therefore, we will use `7za` executables, included in this repository, that support new algorithms (you can find source distributions here: [Windows](https://mcmilk.de/projects/7-Zip-zstd), [Linux/macOS](https://github.com/p7zip-project/p7zip)). Make sure to set `LOCAL_7ZA` path to a binary matching your operating system/architecture.\n",
    "\n",
    "For testing, we will reuse the `testfile` downloaded in the previous section. To parallelize uploading of file chunks, we will use [concurrent futures,](https://docs.python.org/3/library/concurrent.futures.html) a high-level abstraction for concurrent execution. We could also use the previous approach using the `threading` module, however, in the upload case, we're interested in a result of a concurrently executed function (file ID). This is greatly simplified by using futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "LOCAL_7ZA = \"7za_bin/7za_v17.05_mac_arm64\"\n",
    "REMOTE_7ZA = \"7za_bin/7za_v17.05_lnx_x64\"\n",
    "\n",
    "TEST_FILE = \"testfile\"\n",
    "\n",
    "\n",
    "def create_split_archive(\n",
    "    _7za_exe, archive_name, input_path, volume_size_mb=100, delete_existing=True\n",
    "):\n",
    "    for f in glob.glob(f\"{archive_name}.*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"{_7za_exe} a {archive_name} -mx=4 -m0=brotli -mmt16 -aoa -v{volume_size_mb}m {input_path}\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "    )\n",
    "    return glob.glob(f\"{archive_name}.*\")\n",
    "\n",
    "\n",
    "def parallel_upload(api_url, headers, input_files, num_threads=10):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {\n",
    "            executor.submit(file_upload, api_url, headers, f): f for f in input_files\n",
    "        }\n",
    "\n",
    "        file_ids = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            file_id = future.result()\n",
    "            file_ids.append(file_id)\n",
    "\n",
    "    return file_ids\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "split_files = create_split_archive(LOCAL_7ZA, \"testfile.7z\", TEST_FILE)\n",
    "file_ids = parallel_upload(api_url, headers, split_files)\n",
    "print(f\"Parallel upload of split archive: {time.time()-t1} s\")\n",
    "\n",
    "t1 = time.time()\n",
    "file_id = file_upload(api_url, headers, TEST_FILE)\n",
    "print(f\"Upload of decompressed file: {time.time()-t1} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your connection speed and hardware, we should see upload time reduction, for example\n",
    "\n",
    "```\n",
    "Parallel upload of split archive: 66.50943064689636 s\n",
    "Upload of decompressed file: 177.7647111415863 s\n",
    "```\n",
    "\n",
    "We have uploaded our input archive as volumes, so the job that wants to use this archive first needs to reassemble it and decompress it. Since Rescale clusters do not have the `7za` tool supporting the [Brotli algorithm](https://github.com/google/brotli) installed by default, we need to provide it as one of the inputs. Note that we need to upload the executable only once and reuse its file ID for subsequent job submissions (we can also search for existence of a specific file and reuse its ID if found).\n",
    "\n",
    "Let's create a job that will reassemble the archive on a cluster, and calculate a hash of an extracted test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_id = file_upload(api_url, headers, REMOTE_7ZA)\n",
    "file_ids.append(file_id)\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (split archive)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"user_included\", \"version\": \"0\"},\n",
    "            \"command\": \";\".join(\n",
    "                [\n",
    "                    \"cat testfile.7z.* > testfile.7z\",\n",
    "                    \"rm testfile.7z.*\",\n",
    "                    f\"./{os.path.basename(REMOTE_7ZA)} e -mmt16 -aoa testfile.7z\",\n",
    "                    \"echo checksum `shasum -a 512 testfile | awk '{print $1}'`\",\n",
    "                    \"rm testfile*\",\n",
    "                ]\n",
    "            ),\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2, \"walltime\": 1},\n",
    "            \"inputFiles\": [{\"id\": id, \"decompress\": False} for id in file_ids],\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "    \"isLowPriority\": True,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_job_status(api_url, headers, job_id, \"Completed\", verbose=True)\n",
    "\n",
    "# Get the process output file and extract hash calculated by the job\n",
    "output_lines = get_process_output_lines(api_url, headers, job_id)\n",
    "pattern = r\".*checksum ([0-9a-f]*)\"\n",
    "remote_hash = re.match(pattern, output_lines[-3]).group(1)\n",
    "\n",
    "# Compare to hash calculated for a local file\n",
    "local_hash = calculate_file_hash(TEST_FILE)\n",
    "if remote_hash == local_hash:\n",
    "    print(\"Hashes match\")\n",
    "else:\n",
    "    print(\"Hashes do not match\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's free up some cloud storage space by [deleting files](https://engineering.rescale.com/api-docs/#delete-a-file) which have names containing string: `testfile` and were uploaded today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "test_files = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/files/\",\n",
    "    headers=headers,\n",
    "    params={\"search\": \"testfile\"},\n",
    ")\n",
    "\n",
    "files_to_delete = [\n",
    "    f\n",
    "    for f in test_files\n",
    "    if (\n",
    "        datetime.now(timezone.utc)\n",
    "        - datetime.fromisoformat(f[\"dateUploaded\"].replace(\"Z\", \"+00:00\"))\n",
    "    ).days\n",
    "    == 0\n",
    "]\n",
    "\n",
    "for f in files_to_delete:\n",
    "    file_delete_res = requests.delete(\n",
    "        f\"{api_url}/api/v2/files/{f['id']}/\", headers=headers\n",
    "    )\n",
    "    file_delete_res.raise_for_status()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above code will delete all matching input and output files. If deleted files were attached as Job inputs, the link will still be there, but if user tries to clone the job, submission will not work. Deleted files fill need to be removed from inputs.\n",
    "\n",
    "![](README.images/webportal_deleted_input_files.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent transfer reliability\n",
    "\n",
    "Note that the above presented techniques do not implement any reliability measures. This is intentional, as adding retry or partial retry logic would make code less readable. In production code, consider adding logic to recover at least from failures which are now raised with `res.raise_for_status()`. If you feel that an official, reliable Python module for Rescale API based file transfers, published in a [PyPI index](https://pypi.org/) would be a good idea - get in touch with your Rescale contact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Storage devices\n",
    "\n",
    "> TODO\n",
    "\n",
    "## Snapshots, Runs, Tasks and DOE Jobs\n",
    "\n",
    "> TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
