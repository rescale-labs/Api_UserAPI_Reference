{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User API\n",
    "\n",
    "Full documentation of the Rescale APi is available at [https://engineering.rescale.com/api-docs](https://engineering.rescale.com/api-docs). This document builds a story that covers the entire User API surface.\n",
    "\n",
    "## Starting the notebook\n",
    "\n",
    "The only prerequisite to running this tutorial locally is a working installation of Python 3.8+. We're going to work within a virtual environment so that our dependencies are installed locally without affecting the global installation.\n",
    "\n",
    "```\n",
    "$ python -m venv .venv\n",
    "$ . .venv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "$ jupyter notebook README.ipynb\n",
    "```\n",
    "\n",
    "Jupyter Notebook cells need to executed in order as subsequent cells may depend on imports and variables defined in previous cells.\n",
    "\n",
    "### Python and JSON\n",
    "\n",
    "> TODO: Note on how Python dictionaries are 1-1 mapped to JSON.\n",
    "\n",
    "## Authorization\n",
    "\n",
    "The Rescale API authorizes requests using `API Key`. This key is unique per User, per Workspace. A give user may have multiple `API Keys` active - one per Workspace.\n",
    "\n",
    "To generate an `API Key` go to the API section of the User Profile.\n",
    "\n",
    "![](README.images/user_profile_apikey.png)\n",
    "\n",
    "> NOTE: Remember that your `API Key` is a secret. Never store it in a code repository. Never share it with your colleagues.\n",
    "\n",
    "Rescale tools like Rescale CLI look for `apiconfig` authorization configuration file in users home directory.\n",
    "\n",
    "```\n",
    "$HOME/.config/rescale/apiconfig             # Linux\n",
    "%USERPROFILE%\\config\\rescale\\apiconfig      # Windows\n",
    "```\n",
    "\n",
    "We will use this convention to store our credentials. Create the `apiconfig` text file in the aforementioned location and fill it with the following lines\n",
    "\n",
    "```\n",
    "[default]\n",
    "apibaseurl = https://eu.rescale.com\n",
    "apikey = 79a49b3132335a44742e86c9126e5cfaa1ea2489\n",
    "```\n",
    "\n",
    "If you need to execute your script on several platforms you can define alternative profiles (fo example `[us]`). The [config.py](config.py) module is provided for your convenience. You can copy it next to your scripts and use it as a standard way to retrieve credentials. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "api_url, api_key = config.get_profile()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our `apikey` and a base URL for our API calls. Let's make the first call to get our user details. To make REST API calls we will use the [`requests`](https://requests.readthedocs.io/en/latest/) module. It is listed as a dependency in the [`requirements.txt`](requirements.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "results = requests.get(f\"{api_url}/api/v2/users/me/\")\n",
    "if results.status_code != 200:\n",
    "    print(results.status_code)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got status code `401 Unauthorized` which suggests that we're missing our credentials. Let's define `headers` that we will use to authorize future API calls. Instead of checking for result code, let's use a function that raises exception for all statuses that signify lack of success. Finally, we pretty-print the response JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"Token {api_key}\"}\n",
    "results = requests.get(f\"{api_url}/api/v2/users/me/\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(results.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to proceed and build up job submission.\n",
    "\n",
    "## Coretypes and Analyses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job definition requires us to specify software to be attached to a cluster running our job and the hardware (coretype and cores count). Let's [get the list of coretypes](https://engineering.rescale.com/api-docs/#coretypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coretypes_res = requests.get(f\"{api_url}/api/v2/coretypes/\", headers=headers)\n",
    "coretypes_res.raise_for_status()\n",
    "\n",
    "# Display the total count of coretypes and the amount present in the response\n",
    "print(\n",
    "    f\"Count: {coretypes_res.json()['count']}; Length: {len(coretypes_res.json()['results'])}\"\n",
    ")\n",
    "\n",
    "# Display refernece to the next page\n",
    "print(f\"Next page: {coretypes_res.json()['next']}\")\n",
    "\n",
    "# Display a coretype object and its properties\n",
    "pprint(coretypes_res.json()[\"results\"][0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that the response returned total count of coretypes that is larger the the total count of coretype objects in the `results` list. This will happen often for endpoints that potentially return large amounts of data. In such situations results are paged and each page contains a reference to the `next` page.\n",
    "\n",
    "Since paging is common, let's define a function that will loop through all the pages and aggregate objects returned in the `results` list of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_result_pages(url, headers={}, params={}):\n",
    "    results = []\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    results.extend(res.json()[\"results\"])\n",
    "\n",
    "    while res.json()[\"next\"] != None:\n",
    "        res = requests.get(res.json()[\"next\"], headers=headers)\n",
    "        res.raise_for_status()\n",
    "        results.extend(res.json()[\"results\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "coretypes = get_all_result_pages(f\"{api_url}/api/v2/coretypes/\", headers)\n",
    "\n",
    "print(f\"Length: {len(coretypes)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now confident that we fetched all coretypes. For our simple job we will need a general purpose coretype with a low corecount. Let's list coretype `code`s in the `general` category together with their `cores` counts and `processorInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_coretypes = {\n",
    "    c['code']: {'cores': c['cores'], 'processorInfo': c['processorInfo']}\n",
    "    for c in coretypes\n",
    "    if \"general\" in c[\"categoryCodes\"]\n",
    "}\n",
    "\n",
    "pprint(general_coretypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a shortlist we need to decide which coretype to use. Since our test calculation does not have specific requirements, we will use the least expensive option. Let's [fetch coretype prices](https://engineering.rescale.com/api-docs/#list-all-compute-prices), filter prices for on-demand economy (ODE) an `linux` os, link them with general purpose coretypes and get the minium price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prices\n",
    "prices_res = requests.get(f\"{api_url}/api/v2/billing/computeprices/\", headers=headers)\n",
    "prices_res.raise_for_status()\n",
    "\n",
    "# Print sample pricing object\n",
    "pprint(prices_res.json()[-1])\n",
    "\n",
    "# Filter out active ODE linux based coretypes\n",
    "linux_ode_coretypes = {\n",
    "    c[\"coreType\"]: float(c[\"value\"][\"amount\"])\n",
    "    for c in prices_res.json()\n",
    "    if c[\"planType\"].endswith(\"on-demand\")\n",
    "    and c[\"os\"] == \"linux\"\n",
    "    and c[\"isActive\"]\n",
    "    and c[\"coreType\"] in general_coretypes\n",
    "}\n",
    "\n",
    "# Find the least expensive one\n",
    "min_price_coretype = min(linux_ode_coretypes, key=linux_ode_coretypes.get)\n",
    "print(f\"Lowest price coretype: {min_price_coretype} at {linux_ode_coretypes[min_price_coretype]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot market prices are dynamic and therefore your cheapest coretype may change even during a day. Let's assume that `granite` is the coretype we want to use.\n",
    "\n",
    "Now, that we know which coretype to use, we need to select the analysis. In order to build our job create request, we need to know analysis and version codes. Let's query API for the [list of available analyses](https://engineering.rescale.com/api-docs/#analyses) (software tiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses = get_all_result_pages(f\"{api_url}/api/v2/analyses/\", headers)\n",
    "\n",
    "versions_count = 0\n",
    "for a in analyses:\n",
    "    versions_count += len(a[\"versions\"])\n",
    "\n",
    "print(\n",
    "    f\"Total number of analyses: {len(analyses)}, Total number of versions: {versions_count}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample analysis code: {analyses[0]['code']} and version code: {analyses[0]['versions'][0]['versionCode']}\"\n",
    ")\n",
    "\n",
    "pprint(analyses[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your workspace does not have software filters in place, you should see over 700 analyses and over 3200 unique versions. It may be cumbersome to find the code and version that you need. A more practical approach is to create a job using the web portal and then query Rescale API to get the JSON. We will do it in the next section.\n",
    "\n",
    "## Jobs and Files\n",
    "\n",
    "In order to fetch the definition of a previously saved Job, go to the portal and get a Job ID for a job similar to the one you'd like to create via the Rescale API.\n",
    "\n",
    "![](README.images/webportal_jobid.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we captured the Job ID we can [get a specific Job](https://engineering.rescale.com/api-docs/#get-a-specific-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details of a job\n",
    "job_id = \"jNibFc\"\n",
    "\n",
    "results = requests.get(f\"{api_url}/api/v2/jobs/{job_id}\", headers=headers)\n",
    "results.raise_for_status()\n",
    "\n",
    "# Display the analysis section\n",
    "pprint(results.json()[\"jobanalyses\"][0][\"analysis\"])\n",
    "\n",
    "# Display the entire job definition JSON\n",
    "pprint(results.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `analyses` property is a list. This represents a possibility of having multiple software tiles (analyses) attached to a single Job.\n",
    "\n",
    "Rescale API also allows to [list all user jobs](https://engineering.rescale.com/api-docs/#list-all-jobs). Let's count how many jobs completed this year. Additional query parameters allow filtering jobs by:\n",
    "\n",
    "* `state` - one of `(completed, not_completed)`\n",
    "* `job_status` - one of `(PENDING, QUEUED, STARTED, VALIDATED, EXECUTING, COMPLETED, STOPPING, WAITING_FOR_CLUSTER, FORCE_STOP, WAITING_FOR_QUEUE)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers, params={\"state\": \"completed\"}\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "this_year = datetime.today().year\n",
    "\n",
    "# dateInserted does not conform with ISO format - a small replacement is needed\n",
    "this_year_jobs = [\n",
    "    j\n",
    "    for j in jobs\n",
    "    if datetime.fromisoformat(j[\"dateInserted\"].replace(\"Z\", \"+00:00\")).year\n",
    "    == this_year\n",
    "]\n",
    "print(f\"Total jobs: {len(jobs)}, Jobs in {this_year}: {len(this_year_jobs)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow by creating a new Job. We will use `miniconda` analysis to run a [Python script](job_inputs/calculate_pi.py) which estimates the value of π using the the [Leibniz’s formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80). The script requires an [input file](job_inputs/range.inp) which specifies the number of iterations. Estimated value is stored as text in `pi_estimate.res` and in a binary format in `pi_estimate.bin`.\n",
    "\n",
    "> NOTE: If your Workspace does not have the `Miniconda` software enabled, you can use any analysis as all Rescale clusters have a Python interpreter available. CHECK: Do we need a non-licensed software here?\n",
    "\n",
    "It may be the case, that your Rescale setup requires jobs to be submitted against a specific Project. Let's [list available projects](https://engineering.rescale.com/api-docs/#list-projects-available-to-your-user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = get_all_result_pages(f\"{api_url}/api/v2/users/me/projects/\", headers)\n",
    "\n",
    "for p in projects:\n",
    "    print(f\"{p['id']}\\t{p['name']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all needed information about the hardware (`coreType: granite`) and software (`code: miniconda; version: 4.8.4`) we can specify our job definition payload and [create a Job](https://engineering.rescale.com/api-docs/#create-a-job). To keep things simple in this tutorial, we're not handling exceptions raised by our HTTP requests. Production code, should try to recover fro an exception or give end user information on how to proceed. Here, we expect an exception, so we will catch it and display an error, to demonstrate Rescale API returning useful error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your projectId if required/desired\n",
    "project_id = None\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": \"python calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 1},\n",
    "        }\n",
    "    ],\n",
    "    \"projectId\": project_id,\n",
    "}\n",
    "\n",
    "job_save_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "try:\n",
    "    job_save_res.raise_for_status()\n",
    "except:\n",
    "    pprint(job_save_res.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that the coretype we have selected, `granite`, does not support configuration with `1` core per slot. Let's fix it by updating the Job definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"hardware\"][\"coresPerSlot\"] = 2\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "pprint(job_create_res.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was successfully created. We can check whether it is visible in the Rescale web portal.\n",
    "\n",
    "![](README.images/webportal_newjob.png)\n",
    "\n",
    "Job creation operation just saved the job, it has not yet been submitted. In order to [submit a job](https://engineering.rescale.com/api-docs/#submit-a-saved-job) we need to capture its ID and call the `submit` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_submit_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_submit_res.raise_for_status()\n",
    "\n",
    "print(f\"Status code: {job_submit_res.status_code}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status `200` signifies a success. If we go back to the web portal, we will see job statuses changing.\n",
    "\n",
    "![](README.images/webportal_submittedjob.png)\n",
    "\n",
    "We want to wait until the job finishes. Let us poll for [job statuses](https://engineering.rescale.com/api-docs/#list-job-status-history) programmatically. We will define an utility function that polls until a specific Job state is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_for_job_status(job_id, status, interval=30):\n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        job_statuses = get_all_result_pages(\n",
    "            f\"{api_url}/api/v2/jobs/{job_id}/statuses/\", headers=headers\n",
    "        )\n",
    "        sorted_statuses = sorted(\n",
    "            job_statuses,\n",
    "            key=lambda s: datetime.fromisoformat(s[\"statusDate\"]),\n",
    "        )\n",
    "        status_items = [s for s in sorted_statuses if s[\"status\"] == status]\n",
    "        print(\" > \".join([s[\"status\"] for s in sorted_statuses]))\n",
    "\n",
    "        if len(status_items) > 0:\n",
    "            return status_items[0]\n",
    "\n",
    "        time.sleep(interval)\n",
    "\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job went through all the statuses and reached the desired terminal state, however, it seems that the calculation failed. Let's try to figure out why by [listing output files](https://engineering.rescale.com/api-docs/#list-job-output-files). We will search for files that have `output` string in their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": \"output\"}\n",
    ")\n",
    "\n",
    "pprint(output_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_output.log` file is created for all jobs and captures everything written to a console (standard output and error streams). Let's [fetch plaintext contents](https://engineering.rescale.com/api-docs/#get-plaintext-content-of-a-file) of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = output_files[0][\"id\"]\n",
    "\n",
    "file_contents_response = requests.get(\n",
    "    f\"{api_url}/api/v2/files/{file_id}/lines\", headers=headers\n",
    ")\n",
    "file_contents_response.raise_for_status()\n",
    "\n",
    "for line in file_contents_response.json()[\"lines\"]:\n",
    "    print(line, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! All is clear. We forgot to upload input files. Let's [upload files](https://engineering.rescale.com/api-docs/#upload-a-file) and capture their IDs. Since file upload are frequent operation we will encapsulate in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_upload(file_path):\n",
    "    import os\n",
    "\n",
    "    files = [\n",
    "        (\n",
    "            \"file\",\n",
    "            (\n",
    "                os.path.basename(file_path),\n",
    "                open(file_path, \"rb\"),\n",
    "                \"application/octet-stream\",\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    file_upload_res = requests.post(\n",
    "        f\"{api_url}/api/v2/files/contents/\", headers=headers, files=files\n",
    "    )\n",
    "    file_upload_res.raise_for_status()\n",
    "\n",
    "    return file_upload_res.json()[\"id\"]\n",
    "\n",
    "\n",
    "file1_id = file_upload(\"job_inputs/calculate_pi.py\")\n",
    "file2_id = file_upload(\"job_inputs/range.inp\")\n",
    "\n",
    "print(file1_id, file2_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to update our previous job definition by extending the job analysis specification with the `inputFiles` property, and resubmit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition[\"jobanalyses\"][0][\"inputFiles\"] = [{\"id\": file1_id}, {\"id\": file2_id}]\n",
    "pprint(job_definition)\n",
    "\n",
    "job_update_res = requests.patch(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}\", headers=headers, json=job_definition\n",
    ")\n",
    "job_update_res.raise_for_status()\n",
    "pprint(job_update_res.status_code)\n",
    "\n",
    "job_submit_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "try:\n",
    "    job_submit_res.raise_for_status()\n",
    "except:\n",
    "    print(job_submit_res.status_code)\n",
    "    print(job_submit_res.json())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. Although the API responds with `200 OK` for `PATCH`, both of the above operations had no effect as changing or re-submitting a Completed job is not allowed.\n",
    "\n",
    "> NOTE: This is going to be changed to return `400 BAD REQUEST` with an informative error message.\n",
    "\n",
    "Since we can not reuse our failed job (the public API does not support a clone operation) let's [delete it](https://engineering.rescale.com/api-docs/#delete-a-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_job_res = requests.delete(f\"{api_url}/api/v2/jobs/{job_id}\", headers=headers)\n",
    "delete_job_res.raise_for_status()\n",
    "\n",
    "print(delete_job_res.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create new job with files attached, submit it and wait till it completes. With the new Job definition, we also specify that we want to run the job using On-Demand Economy (spot market) instances (the `isLowPriority` property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (with inputs)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": \"python calculate_pi.py range.inp\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2},\n",
    "            \"inputFiles\": [{\"id\": file1_id}, {\"id\": file2_id}],\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "    \"isLowPriority\": True\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of output files and download them all. Again, Downloading files is a frequent operation, let's encapsulate it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_file(api_url, headers, file_id, out_dir, file_name):\n",
    "    response = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{file_id}/contents\", headers=headers\n",
    "    )\n",
    "\n",
    "    chunk_size = 4096\n",
    "    with open(Path(out_dir, file_name), \"wb\") as fd:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "out_dir = \"job_outputs\"\n",
    "Path(out_dir).mkdir(exist_ok=True)\n",
    "\n",
    "files = get_all_result_pages(f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers)\n",
    "pprint(files)\n",
    "\n",
    "for f in files:\n",
    "    download_file(api_url, headers, f[\"id\"], out_dir, f[\"name\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale Files metadata contains the `typeId` property which can have one of the following values.\n",
    "\n",
    "| id | type                 |\n",
    "| -- | -------------------- |\n",
    "| 1  | INPUT_FILE           |\n",
    "| 2  | TEMPLATE_FILE        |\n",
    "| 3  | PARAM_FILE           |\n",
    "| 4  | SCRIPT               |\n",
    "| 5  | OUTPUT_FILE          |\n",
    "| 8  | CASE_FILE            |\n",
    "| 10 | TEMPORARY_FILE       |\n",
    "| 11 | CHECKPOINT_ARCHIVE   |\n",
    "| 12 | SNAPSHOT_FILE        |\n",
    "\n",
    "We can [retrieve metadata](https://engineering.rescale.com/api-docs/#get-metadata-of-a-file) for each file and compare checksums with downloaded files. We fetched this information already when listing job files, here we demonstrate using the `/files` resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "for f in files:\n",
    "    file_metadata_res = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{f['id']}/\", headers=headers\n",
    "    )\n",
    "    file_metadata_res.raise_for_status()\n",
    "    remote_hash = file_metadata_res.json()\n",
    "\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "    chunk_size = 4096\n",
    "\n",
    "    with open(Path(out_dir, f[\"name\"]), \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(chunk_size), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    if hash_sha512.hexdigest() == f[\"fileChecksums\"][0][\"fileHash\"]:\n",
    "        print(f\"OK\\t{f['name']}\")\n",
    "    else:\n",
    "        print(f\"FAIL\\t{f['name']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's clean up our job by deleting it together with input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_delete_res = requests.delete(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}\",\n",
    "    headers=headers,\n",
    "    params={\"deleteInputFiles\": True},\n",
    ")\n",
    "job_delete_res.raise_for_status()\n",
    "print(job_create_res.status_code)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing file transfers\n",
    "\n",
    "### Archiving files before upload\n",
    "\n",
    "> TODO: Create zip file before submit \n",
    "\n",
    "### Parallel file download\n",
    "\n",
    "Rescale API file download endpoint supports [HTTP Range requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). This allows client code to split download into several chunks and download these in parallel.\n",
    "\n",
    "Let's start from submitting a job that will generate a 1GB result file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_NAME = \"testfile\"\n",
    "\n",
    "job_definition = {\n",
    "    \"name\": \"Rescale UserApi Reference Tutorial (1GB result file)\",\n",
    "    \"jobanalyses\": [\n",
    "        {\n",
    "            \"analysis\": {\"code\": \"miniconda\", \"version\": \"4.8.4\"},\n",
    "            \"command\": f\"openssl rand -out ${TEST_FILE_NAME} -base64 792917038\",\n",
    "            \"hardware\": {\"coreType\": \"granite\", \"coresPerSlot\": 2},\n",
    "        }\n",
    "    ],\n",
    "    \"project_id\": project_id,\n",
    "}\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/\", headers=headers, json=job_definition\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "job_id = job_create_res.json()[\"id\"]\n",
    "\n",
    "job_create_res = requests.post(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/submit/\", headers=headers\n",
    ")\n",
    "job_create_res.raise_for_status()\n",
    "\n",
    "poll_for_job_status(job_id, \"Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command of the above Job used `openssl` to create a random file that is now stored in cloud storage. The code below defines several functions that work together to download a file in chucks, each in its own thread. Consult the code comments for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import threading\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "Chunk = namedtuple(\"Chunk\", [\"idx\", \"start_byte\", \"end_byte\"])\n",
    "FILE_IO_CHUNK_SIZE = 4096\n",
    "\n",
    "\n",
    "def _calculate_file_hash(file_path):\n",
    "    hash_sha512 = hashlib.sha512()\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(FILE_IO_CHUNK_SIZE), b\"\"):\n",
    "            hash_sha512.update(chunk)\n",
    "\n",
    "    return hash_sha512.hexdigest()\n",
    "\n",
    "\n",
    "def _concatenate_files(output_file, *input_files, delete_chunks=True):\n",
    "    with open(output_file, \"wb\") as output:\n",
    "        for file_name in input_files:\n",
    "            with open(file_name, \"rb\") as input_file:\n",
    "                chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "                while chunk:\n",
    "                    output.write(chunk)\n",
    "                    chunk = input_file.read(FILE_IO_CHUNK_SIZE)\n",
    "            if delete_chunks:\n",
    "                os.remove(file_name)\n",
    "\n",
    "\n",
    "def _chunk_download(api_url, headers, file_id, dir_path, file_chunk: Chunk):\n",
    "    headers[\"Range\"] = f\"bytes={file_chunk.start_byte}-{file_chunk.end_byte}\"\n",
    "    response = requests.get(\n",
    "        f\"{api_url}/api/v2/files/{file_id}/contents\", headers=headers\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(Path(dir_path, f\"{file_id}.{file_chunk.idx}\"), \"wb\") as fd:\n",
    "        for chunk in response.iter_content(FILE_IO_CHUNK_SIZE):\n",
    "            fd.write(chunk)\n",
    "\n",
    "\n",
    "def parallel_download(\n",
    "    api_url,\n",
    "    headers,\n",
    "    file_id,\n",
    "    dir_path=\".\",\n",
    "    file_name=None,\n",
    "    num_threads=10,\n",
    "    threshold_mb=50,\n",
    "):\n",
    "    # get file size to determine whether splitting makes sense\n",
    "    response = requests.get(f\"{api_url}/api/v2/files/{file_id}/\", headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    decrypted_size = response.json()[\"decryptedSize\"]\n",
    "\n",
    "    # Check if size is above threshold, if not download in one piece\n",
    "    num_threads = num_threads if decrypted_size > threshold_mb * 1048576 else 1\n",
    "\n",
    "    file_hash = response.json()[\"fileChecksums\"][0][\"fileHash\"]\n",
    "    file_name = file_name if file_name != None else response.json()[\"name\"]\n",
    "\n",
    "    chunk_size = int(decrypted_size / num_threads)\n",
    "    threads = []\n",
    "    for idx in range(0, num_threads):\n",
    "        start_byte = idx * chunk_size\n",
    "\n",
    "        # Make sure the last chunk fetches all remaining bytes\n",
    "        end_byte = (\n",
    "            (idx + 1) * chunk_size - 1 if idx != num_threads - 1 else decrypted_size + 1\n",
    "        )\n",
    "\n",
    "        t = threading.Thread(\n",
    "            target=_chunk_download,\n",
    "            args=(\n",
    "                api_url,\n",
    "                headers,\n",
    "                file_id,\n",
    "                dir_path,\n",
    "                Chunk(idx, start_byte, end_byte),\n",
    "            ),\n",
    "        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    _concatenate_files(\n",
    "        file_name, *[f\"{file_id}.{idx}\" for idx in range(0, num_threads)]\n",
    "    )\n",
    "\n",
    "    # Compare file hashes to make sure downloaded file is not corrupted.\n",
    "    filehash = _calculate_file_hash(file_name)\n",
    "    if filehash != file_hash:\n",
    "        raise Exception(\"File hashes not equal. Corrupted file download.\")\n",
    "\n",
    "\n",
    "# Get test file ID\n",
    "output_files_res = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/files/\", headers, params={\"search\": TEST_FILE_NAME}\n",
    ")\n",
    "test_file_id = output_files_res[0][\"id\"]\n",
    "\n",
    "# Compare download speed\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "parallel_download(api_url, headers, test_file_id, num_threads=10)\n",
    "print(f\"Time with 10 threads: {time.time() - t1} s\")\n",
    "\n",
    "t1 = time.time()\n",
    "parallel_download(api_url, headers, test_file_id, num_threads=1)\n",
    "print(f\"Time with 1 thread: {time.time() - t1} s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your connection speed and hardware we should see download time reduction, for example\n",
    "\n",
    "```\n",
    "Time with 10 threads: 35.551738023757935 s\n",
    "Time with 1 thread: 124.59398603439331 s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel upload with archive volumes\n",
    "\n",
    "> TODO: Split archive into volumes, upload in parallel, merge in command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage devices\n",
    "\n",
    "> TODO\n",
    "\n",
    "## Snapshots, Runs, Tasks and DOE Jobs\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters\n",
    "\n",
    "> TODO: /v2/jobs/{job_id}/instances/ /v2/jobs/{job_id}/cluster_statuses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_statuses = get_all_result_pages(\n",
    "    f\"{api_url}/api/v2/jobs/{job_id}/cluster_statuses/\", headers\n",
    ")\n",
    "\n",
    "sorted_statuses = sorted(\n",
    "    cluster_statuses,\n",
    "    key=lambda s: datetime.fromisoformat(s[\"statusDate\"].replace(\"Z\", \"+00:00\")),\n",
    ")\n",
    "for s in sorted_statuses:\n",
    "    print(f\"{s['statusDate']}\\t{s['status']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
